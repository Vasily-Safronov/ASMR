{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Speech Recognition with Speaker Diarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nemo.collections.asr as nemo_asr\n",
    "from nemo.collections.asr.parts.utils.decoder_timestamps_utils import ASRDecoderTimeStamps\n",
    "from nemo.collections.asr.parts.utils.diarization_utils import OfflineDiarWithASR\n",
    "\n",
    "# import numpy as np\n",
    "from IPython.display import Audio, display\n",
    "import librosa\n",
    "import os\n",
    "import wget\n",
    "\n",
    "# import nemo\n",
    "import glob\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "from audio_extract import extract_audio\n",
    "import os, errno\n",
    "from pydub import AudioSegment\n",
    "import numpy as np \n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from omegaconf import OmegaConf\n",
    "import json\n",
    "\n",
    "import logging\n",
    "logging.getLogger('nemo_logger').setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = os.getcwd()\n",
    "data_dir = os.path.join(ROOT, 'data')\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "videos_dir = os.path.join(data_dir, 'videos')\n",
    "os.makedirs(videos_dir, exist_ok=True)\n",
    "\n",
    "audios_dir = os.path.join(data_dir, 'audios')\n",
    "os.makedirs(audios_dir, exist_ok=True)\n",
    "\n",
    "text_dir = os.path.join(data_dir,'text')\n",
    "os.makedirs(text_dir, exist_ok=True)\n",
    "\n",
    "list_videos = os.listdir(videos_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract audio from video\n",
    "for video_file in list_videos:\n",
    "    try:\n",
    "        extract_audio(input_path=os.path.join(videos_dir, video_file), \n",
    "                    output_path=os.path.join(audios_dir, video_file.split('.')[0] + \".wav\") , \n",
    "                    output_format='wav')\n",
    "        \n",
    "        audio = AudioSegment.from_wav(os.path.join(audios_dir, video_file.split('.')[0] + \".wav\"))\n",
    "        split_audio = audio\n",
    "        split_audio = split_audio.set_frame_rate(16000)                \n",
    "        split_audio = split_audio.set_channels(1)\n",
    "        split_audio.export(os.path.join(audios_dir, video_file.split('.')[0] + \".wav\"), format=\"wav\")\n",
    "    except:\n",
    "        print(\"файл уже существует\")\n",
    "\n",
    "audio_file_list = glob.glob(f\"{audios_dir}/*.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter setting for ASR and diarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dir = os.path.join(data_dir,'config')\n",
    "os.makedirs(config_dir, exist_ok=True)\n",
    "\n",
    "text_dir = os.path.join(data_dir,'text')\n",
    "os.makedirs(text_dir, exist_ok=True)\n",
    "\n",
    "DOMAIN_TYPE = \"meeting\" # Can be meeting or telephonic based on domain type of the audio file\n",
    "CONFIG_FILE_NAME = f\"diar_infer_{DOMAIN_TYPE}.yaml\"\n",
    "\n",
    "CONFIG_URL = f\"https://raw.githubusercontent.com/NVIDIA/NeMo/main/examples/speaker_tasks/diarization/conf/inference/{CONFIG_FILE_NAME}\"\n",
    "\n",
    "if not os.path.exists(os.path.join(config_dir, CONFIG_FILE_NAME)):\n",
    "    CONFIG = wget.download(CONFIG_URL, config_dir)\n",
    "else:\n",
    "    CONFIG = os.path.join(config_dir,CONFIG_FILE_NAME)\n",
    "\n",
    "cfg = OmegaConf.load(CONFIG)\n",
    "pretrained_speaker_model='titanet_large'\n",
    "cfg.diarizer.out_dir = data_dir # Directory to store intermediate files and prediction outputs\n",
    "cfg.diarizer.speaker_embeddings.model_path = pretrained_speaker_model\n",
    "cfg.diarizer.clustering.parameters.oracle_num_speakers = False\n",
    "\n",
    "# Using Neural VAD and Conformer ASR \n",
    "cfg.diarizer.vad.model_path = 'vad_multilingual_marblenet'\n",
    "cfg.diarizer.asr.model_path = 'stt_ru_quartznet15x5'\n",
    "cfg.diarizer.oracle_vad = False # ----> Not using oracle VAD \n",
    "cfg.diarizer.asr.parameters.asr_based_vad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = os.getcwd()\n",
    "data_dir_local = os.path.join(data_dir,'cache_samples_data')\n",
    "os.makedirs(data_dir_local, exist_ok=True)\n",
    "\n",
    "for audio_filename in audio_file_list:\n",
    "\n",
    "    meta = {\n",
    "    'audio_filepath': audio_filename, \n",
    "    'offset': 0, \n",
    "    'duration':None,\n",
    "    'label': 'infer', \n",
    "    'text': '-', \n",
    "    'num_speakers': None, \n",
    "    'rttm_filepath': None, \n",
    "    'uem_filepath' : None\n",
    "    }\n",
    "\n",
    "    with open(os.path.join(config_dir,'input_manifest.json'),'w') as fp:\n",
    "        json.dump(meta,fp)\n",
    "        fp.write('\\n')\n",
    "\n",
    "    cfg.diarizer.manifest_filepath = os.path.join(config_dir,'input_manifest.json')\n",
    "\n",
    "    asr_decoder_ts = ASRDecoderTimeStamps(cfg.diarizer)\n",
    "    asr_model = asr_decoder_ts.set_asr_model()\n",
    "    word_hyp, word_ts_hyp = asr_decoder_ts.run_ASR(asr_model)\n",
    "\n",
    "    asr_diar_offline = OfflineDiarWithASR(cfg.diarizer)\n",
    "    asr_diar_offline.word_ts_anchor_offset = asr_decoder_ts.word_ts_anchor_offset\n",
    "\n",
    "    diar_hyp, diar_score = asr_diar_offline.run_diarization(cfg, word_ts_hyp)\n",
    "\n",
    "    sum_text = ''\n",
    "    for data in diar_hyp[audio_filename.split('/')[-1][:-4]]:\n",
    "        speaker = data.split('speaker_')[1]\n",
    "        t1, t2 = float(data.split(' ')[0]) * 1000, float(data.split(' ')[1]) * 1000\n",
    "        audio = AudioSegment.from_wav(audio_filename)\n",
    "        split_audio = audio[t1:t2]\n",
    "        # split_audio = split_audio.set_frame_rate(16000)                \n",
    "        # split_audio = split_audio.set_channels(1)\n",
    "        split_audio = split_audio.split_to_mono()[0]\n",
    "        split_audio.export(os.path.join(data_dir_local, 'sample.wav'), format=\"wav\")\n",
    "        path_to_audio_sample = os.path.join(data_dir_local, 'sample.wav')\n",
    "\n",
    "        try:\n",
    "            asr_model = nemo_asr.models.EncDecHybridRNNTCTCBPEModel.from_pretrained(model_name=\"nvidia/stt_ru_fastconformer_hybrid_large_pc\", map_location='cuda', )\n",
    "            text = asr_model.transcribe([path_to_audio_sample], batch_size=8)[0]\n",
    "            if type(text) == list:\n",
    "                sum_text += 'speaker_' + speaker + ' ' + text[0] + '\\n'\n",
    "            else:\n",
    "                sum_text += 'speaker_' + speaker + ' ' + text + '\\n'\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    with open(os.path.join(text_dir, f\"{audio_filename.split('/')[-1][:-4]}текст.txt\"), \"w\") as file:\n",
    "        file.write(sum_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
